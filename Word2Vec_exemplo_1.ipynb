{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_exemplo_1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1bZ75Zg08E9qCuR01BxQJrwS1rOICyOS3",
      "authorship_tag": "ABX9TyN2MtcHrrSwKy/vTi1xNHCP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EddyGiusepe/Word2Vec_exemplo_1/blob/main/Word2Vec_exemplo_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4YsURrPJ1WX"
      },
      "source": [
        "# <h2 align='center'>**Python | Word Embedding using Word2Vec**</h2> \n",
        "\n",
        "\\\\\n",
        "**Cientista de Dados Jr.:**  Dr.Eddy Giusepe Chirinos Isidro\n",
        "\n",
        "\n",
        "link de estudo: [CLICK AQUI](https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESgBT44KJ1UA"
      },
      "source": [
        "<font color=\"yellow\">Word Embedding</font> é uma técnica de modelagem de linguagem usada para mapear palavras em vetores de números reais. Representa palavras ou frases no espaço vetorial com várias dimensões. Embeddings de palavras podem ser gerados usando vários métodos, como redes neurais, matriz de co-ocorrência, modelos probabilísticos, etc.\n",
        "\n",
        "\n",
        "\\\\\n",
        "<font color=\"yellow\">Word2Vec</font> consiste em modelos para gerar incorporação de palavras. Esses modelos são redes neurais superficiais de duas camadas com uma camada de entrada, uma camada oculta e uma camada de saída. Word2Vec utiliza duas arquiteturas:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aekvgtzJ1RS"
      },
      "source": [
        "**CBOW (Continuous Bag of Words):**\n",
        "\n",
        "o modelo CBOW prevê a palavra atual dadas as palavras de contexto dentro de uma janela específica. A camada de entrada contém as palavras de contexto e a camada de saída contém a palavra atual. A camada oculta contém o número de dimensões nas quais queremos representar a palavra atual presente na camada de saída.\n",
        "\n",
        "\n",
        "\\\\\n",
        "**Skip Gram:** \n",
        "\n",
        "Skip grama prevê as palavras de contexto circundantes dentro de uma janela específica dada a palavra atual. A camada de entrada contém a palavra atual e a camada de saída contém as palavras do contexto. A camada oculta contém o número de dimensões nas quais queremos representar a palavra atual presente na camada de entrada.\n",
        "\n",
        "\\\\\n",
        "A ideia básica da incorporação de palavras é que palavras que ocorrem em contextos semelhantes tendem a estar mais próximas umas das outras no espaço vetorial. Para gerar vetores de palavras em Python, os módulos necessários são nltk e gensim.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKCoNiEPJu3N",
        "outputId": "fe5dbb67-fca0-46d0-f4a0-3aeee83c2b2d"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfvpxKeoM57Z",
        "outputId": "825e0ce4-d3ea-4e2b-d63f-65f3c3918e02"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtuhgC4xNmyO"
      },
      "source": [
        "## Python program to generate word vectors using Word2Vec\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-X9T37FNAC3",
        "outputId": "21a7c48b-7e7f-4da5-e2d2-bbf894f1e8b1"
      },
      "source": [
        "# Importing all necessary modules\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import warnings\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Resource punkt not found. Please use the NLTK Downloader to obtain the resource:\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NABHFd8FN-d_"
      },
      "source": [
        "warnings.filterwarnings(action = 'ignore')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmWVY_SrOBWm"
      },
      "source": [
        "#  Reads ‘alice.txt’ file\n",
        "\n",
        "sample = open(\"/content/drive/MyDrive/2_DEEP LEARNING REDES NEURAIS -- Jorge/text_classification_Word2Vec_Eddy/alice.txt\", \"r\")\n",
        "s = sample.read()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dJo3AaNQYbE"
      },
      "source": [
        "# Replaces escape character with space\n",
        "f = s.replace(\"\\n\", \" \")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC4Wo1iYTG4j"
      },
      "source": [
        "data = list()\n",
        "\n",
        "# Iterate through each sentence in the file\n",
        "for i in sent_tokenize(f):\n",
        "  temp = list()\n",
        "  # tokenize the sentence into words\n",
        "  for j in word_tokenize(i):\n",
        "    temp.append(j.lower())\n",
        "\n",
        "    data.append(temp)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l41gxHeZRDdI"
      },
      "source": [
        "# Create CBOW model\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1, \n",
        "                              size = 1000, window = 5)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FBuMZfaU8Ts",
        "outputId": "3e804fc3-a946-455c-a183-744cfe151113"
      },
      "source": [
        "# Print results\n",
        "print(\"Cosine similarity between 'alice' \" + \"and 'wonderland' - CBOW : \", model1.similarity('alice', 'wonderland'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.06948266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPTHake0VW9L",
        "outputId": "6136d0b5-ed4e-4676-859e-b82fbbfcd187"
      },
      "source": [
        "print(\"Cosine similarity between 'alice' \" + \"and 'machines' - CBOW : \", model1.similarity('alice', 'machines'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'alice' and 'machines' - CBOW :  -0.0037568505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aVkRZ-WVxnJ"
      },
      "source": [
        "# Create Skip Gram model\n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100,\n",
        "                                             window = 5, sg = 1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlgktPSTV6TT",
        "outputId": "a8543f32-7ac6-4862-c329-242e4c93eaf1"
      },
      "source": [
        "# Print results\n",
        "print(\"Cosine similarity between 'alice' \" +\n",
        "          \"and 'wonderland' - Skip Gram : \",\n",
        "    model2.similarity('alice', 'wonderland'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.12857467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q338gHygWBLa",
        "outputId": "6a11fb74-6634-4905-f409-22e2d42dc423"
      },
      "source": [
        "print(\"Cosine similarity between 'alice' \" +\n",
        "            \"and 'machines' - Skip Gram : \",\n",
        "      model2.similarity('alice', 'machines'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.1469699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RsyNY8AWqv7"
      },
      "source": [
        "Output indicates the cosine similarities between word vectors ‘alice’, ‘wonderland’ and ‘machines’ for different models. One interesting task might be to change the parameter values of ‘size’ and ‘window’ to observe the variations in the cosine similarities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsh2i19xWGO-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}